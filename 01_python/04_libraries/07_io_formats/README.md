# IO Formats (Python Libraries â€” 07)

This module covers **production-grade data input/output architecture**
using Python.

Rather than demonstrating basic file reading examples,
this module treats I/O as a **data engineering boundary layer**
between raw storage systems and analytical / ML pipelines.

It focuses on how structured and semi-structured data flows across:

- Flat files (CSV, Excel)
- Semi-structured formats (JSON)
- Columnar storage (Parquet)
- Relational databases (SQL)

---

ë³¸ ëª¨ë“ˆì€ Python ê¸°ë°˜ì˜ **ì‹¤ë¬´ ìˆ˜ì¤€ ë°ì´í„° ì…ì¶œë ¥(I/O) ì•„í‚¤í…ì²˜ ì„¤ê³„**ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.

ë‹¨ìˆœ íŒŒì¼ ì…ì¶œë ¥ì´ ì•„ë‹ˆë¼,  
íŒŒì¼ ì‹œìŠ¤í…œÂ·ì»¬ëŸ¼í˜• ì €ì¥ì†ŒÂ·ê´€ê³„í˜• DBì™€  
ë¶„ì„/ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ì„ ì—°ê²°í•˜ëŠ” **ë°ì´í„° ì¸í„°í˜ì´ìŠ¤ ë ˆì´ì–´**ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤.

---

## ğŸ¯ Learning Objectives

After completing this module, you will be able to:

- Safely read and write CSV / Excel files with schema control
- Normalize and serialize JSON data for analytics pipelines
- Leverage Parquet for efficient columnar storage
- Connect pandas with SQL databases securely
- Implement chunk-based processing for large datasets
- Use parameter binding to prevent SQL injection
- Design idempotent, reproducible data ingestion workflows

ë³¸ ëª¨ë“ˆ ì™„ë£Œ í›„ ë‹¤ìŒì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- ìŠ¤í‚¤ë§ˆ í†µì œê°€ ê°€ëŠ¥í•œ CSV/Excel ì…ì¶œë ¥
- JSON ì •ê·œí™” ë° ì§ë ¬í™” ì²˜ë¦¬
- Parquet ê¸°ë°˜ ê³ ì„±ëŠ¥ ì»¬ëŸ¼ ì €ì¥ ì „ëµ ì´í•´
- pandas â†” SQL ì•ˆì „ ì—°ê²° ì„¤ê³„
- ëŒ€ìš©ëŸ‰ chunk ì²˜ë¦¬ êµ¬í˜„
- SQL injection ë°©ì§€ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ë°”ì¸ë”©
- ë©±ë“±ì ì´ê³  ì¬í˜„ ê°€ëŠ¥í•œ ë°ì´í„° ì ì¬ êµ¬ì¡° ì„¤ê³„

---

# ğŸ“‚ Files & Progress

---

## âœ… Day 44 â€” CSV & Excel I/O  
`01_csv_excel.py`

### Core Coverage

- `pd.read_csv()` advanced configuration
- dtype enforcement
- Date parsing strategies
- Missing value normalization
- Excel multi-sheet read/write patterns
- Controlled export formatting

### í•œêµ­ì–´ ìš”ì•½

- CSV ê³ ê¸‰ ì˜µì…˜ ì„¤ê³„
- dtype ë° ë‚ ì§œ íŒŒì‹± ì „ëµ
- ê²°ì¸¡ì¹˜ ì •ê·œí™” ì²˜ë¦¬
- Excel ë‹¤ì¤‘ ì‹œíŠ¸ ì…ì¶œë ¥
- ì•ˆì •ì  export í¬ë§· ì„¤ê³„

---

## âœ… Day 45 â€” JSON & Parquet  
`02_json_parquet.py`

### Core Coverage

- JSON normalization with `pd.json_normalize()`
- Nested structure flattening
- Parquet read/write patterns
- Columnar storage performance rationale
- Serialization best practices

### í•œêµ­ì–´ ìš”ì•½

- JSON êµ¬ì¡° ì •ê·œí™”
- ì¤‘ì²© ë°ì´í„° í‰íƒ„í™”
- Parquet ì €ì¥ ì „ëµ
- ì»¬ëŸ¼í˜• ì €ì¥ì†Œ ì„±ëŠ¥ ì¥ì 
- ë°ì´í„° ì§ë ¬í™” ì„¤ê³„ ê°œë…

---

## âœ… Day 46 â€” SQL Read/Write Architecture  
`03_sql_read_write.py`

### Core Coverage

- `pandas.read_sql_query`
- `DataFrame.to_sql`
- Secure parameter binding (`:param`)
- Transaction control (`with connection`)
- Chunked batch inserts
- Schema validation pattern
- Safe identifier handling
- SQLite demo for reproducibility
- Production extension via SQLAlchemy (Postgres/MySQL ready)

### í•œêµ­ì–´ ìš”ì•½

- pandas â†” SQL ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„
- íŒŒë¼ë¯¸í„° ë°”ì¸ë”© ê¸°ë°˜ ë³´ì•ˆ ì²˜ë¦¬
- íŠ¸ëœì­ì…˜ ê¸°ë°˜ ë°ì´í„° ì ì¬
- ëŒ€ìš©ëŸ‰ batch insert ì „ëµ
- ìŠ¤í‚¤ë§ˆ ê²€ì¦ íŒ¨í„´
- SQL ì‹ë³„ì ì•ˆì „ ì²˜ë¦¬
- SQLite ê¸°ë°˜ ì¬í˜„ ê°€ëŠ¥í•œ ë°ëª¨
- ì‹¤ë¬´ DB í™•ì¥ ì„¤ê³„ ê°œë…

---

# ğŸ§  Architectural Perspective

I/O is not just data loading.

It is a **system boundary definition layer**.

Poorly designed I/O leads to:

- Schema drift
- Data corruption
- Performance bottlenecks
- Security vulnerabilities
- Non-reproducible pipelines

Well-designed I/O guarantees:

- Reproducibility
- Auditability
- Scalability
- Security
- Stable downstream analytics

---

ë°ì´í„° ì…ì¶œë ¥ì€ ë‹¨ìˆœ íŒŒì¼ ì²˜ë¦¬ ë‹¨ê³„ê°€ ì•„ë‹™ë‹ˆë‹¤.  
**ì‹œìŠ¤í…œ ê²½ê³„ë¥¼ ì •ì˜í•˜ëŠ” í•µì‹¬ ì„¤ê³„ ê³„ì¸µ**ì…ë‹ˆë‹¤.

ì˜ ì„¤ê³„ëœ I/O êµ¬ì¡°ëŠ”  
ì¬í˜„ì„±, ì•ˆì •ì„±, í™•ì¥ì„±, ë³´ì•ˆì„±ì„ ë™ì‹œì— ë³´ì¥í•©ë‹ˆë‹¤.

---

# ğŸ”„ Conceptual Data Flow

```text
External Storage
        â†“
File / Database Interface
        â†“
Validation & Normalization
        â†“
Structured DataFrame
        â†“
Analytics / ML Pipeline