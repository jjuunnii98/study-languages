"""
Day 39 â€” Linear Models (Regression & Classification)

This file covers:
1. Linear Regression
2. Logistic Regression
3. Regularization (Ridge, Lasso concept)

Linear models are the foundation of machine learning.
They are simple, interpretable, and often strong baselines.

ì´ íŒŒì¼ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ ê°€ì¥ ê¸°ë³¸ì´ ë˜ëŠ”
ì„ í˜• ëª¨ë¸ì„ ë‹¤ë£¬ë‹¤.

- ì„ í˜• íšŒê·€
- ë¡œì§€ìŠ¤í‹± íšŒê·€
- ê·œì œ ê°œë…
"""

# --------------------------------------------------
# 1ï¸âƒ£ Imports
# --------------------------------------------------

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge
from sklearn.metrics import r2_score, accuracy_score

# --------------------------------------------------
# 2ï¸âƒ£ Linear Regression Example
# --------------------------------------------------

"""
Regression Task:
Predict continuous values
"""

# Synthetic regression data
np.random.seed(42)
X = np.random.rand(100, 2)
y = 3 * X[:, 0] + 5 * X[:, 1] + np.random.randn(100)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("=== Linear Regression ===")
print("RÂ² Score:", r2_score(y_test, y_pred))
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)

"""
ğŸ“Œ í•œêµ­ì–´ ì„¤ëª…

- coef_: ê° ë³€ìˆ˜ì˜ ì˜í–¥ë ¥ (ê¸°ìš¸ê¸°)
- intercept_: ì ˆí¸
- RÂ²: ëª¨ë¸ ì„¤ëª…ë ¥

ì„ í˜• íšŒê·€ëŠ” í•´ì„ì´ ëª…í™•í•˜ê³ 
ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ë¡œ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤.
"""

# --------------------------------------------------
# 3ï¸âƒ£ Logistic Regression Example
# --------------------------------------------------

"""
Classification Task:
Binary classification
"""

# Synthetic classification data
X_cls = np.random.randn(200, 2)
y_cls = (X_cls[:, 0] + X_cls[:, 1] > 0).astype(int)

X_train, X_test, y_train, y_test = train_test_split(
    X_cls, y_cls, test_size=0.2, random_state=42
)

clf = LogisticRegression()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print("\n=== Logistic Regression ===")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Coefficients:", clf.coef_)

"""
ğŸ“Œ í•œêµ­ì–´ ì„¤ëª…

- Logistic Regressionì€ í™•ë¥ ì„ ì˜ˆì¸¡
- ì„ í˜• ê²°í•© â†’ sigmoid í•¨ìˆ˜ â†’ í™•ë¥ 
- í•´ì„ ê°€ëŠ¥í•œ ë¶„ë¥˜ ëª¨ë¸
"""

# --------------------------------------------------
# 4ï¸âƒ£ Regularization Example (Ridge)
# --------------------------------------------------

"""
Regularization helps prevent overfitting.
Ridge adds L2 penalty to coefficients.
"""

ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

print("\n=== Ridge Regularization ===")
print("Coefficients:", ridge.coef_)

"""
ğŸ“Œ í•œêµ­ì–´ ì„¤ëª…

- alpha â†‘ â†’ ê³„ìˆ˜ shrinkage ì¦ê°€
- ê³¼ì í•© ë°©ì§€
- ë³€ìˆ˜ ë§ì„ ë•Œ ì•ˆì •ì 

ì„ í˜• ëª¨ë¸ì€:
- í•´ì„ ê°€ëŠ¥
- ë¹ ë¦„
- ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ ê°•ë ¥í•¨
"""